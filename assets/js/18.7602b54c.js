(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{353:function(t,a,i){"use strict";i.r(a);var s=i(4),o=Object(s.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("机器学习概括来说，"),a("strong",[t._v("机器学习就是让机器具备找一个函式的能力")]),t._v("。")]),t._v(" "),a("h1",{attrs:{id:"different-types-of-functions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#different-types-of-functions"}},[t._v("#")]),t._v(" Different types of Functions")]),t._v(" "),a("ul",[a("li",[a("p",[a("strong",[t._v("Regression")]),t._v("（回归）")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("输出是一个数值（scalar）")]),t._v(" "),a("blockquote",[a("p",[t._v("例如，今天要机器做的事情,是预测未来某一个时间的,PM2.5的数值。输入可能是种种跟预测PM2.5,有关的指数,这一个函式可以拿这些数值当作输入,输出明天中午的PM2.5的数值,那这一个找这个函式的任务,叫作Regression。")])])])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("Classification")]),t._v("（分类）")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("要机器做选择题。人类先备好一些选项（也叫类别）（classes），从设定好的选项中选择一个当做输出，这个任务就叫做 Classification。")]),t._v(" "),a("blockquote",[a("p",[t._v("例如，gmail account裡面有一个函式,帮我们侦测一封邮件,是不是垃圾邮件。")]),t._v(" "),a("p",[t._v("alpha go本身也是一个 Classification 的问题。")])])])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("Structured Learning")])]),t._v(" "),a("ul",[a("li",[t._v("机器今天不只是要做选择题，不只是输出一个数字，还要产生一个有结构的物件。机器画一张图 写一篇文章,这种叫机器产生有结构的东西的问题,就叫作Structured Learning。即要机器学会创造。")])])])]),t._v(" "),a("h1",{attrs:{id:"case-study"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#case-study"}},[t._v("#")]),t._v(" Case Study")]),t._v(" "),a("p",[t._v("举例说明机器怎么找一个函式。")]),t._v(" "),a("p",[t._v("找一个函式,这个函式的输入是youtube上面一个频道过往所有的资讯，输出就是预测明天这个频道可能的总观看次数。")]),t._v(" "),a("p",[a("strong",[t._v("找这个函式的过程，分为三步：")])]),t._v(" "),a("ol",[a("li",[t._v("写出一个带有未知参数的函式；2. 定义一个叫做Loss的function；3. 解一个Optimization的问题找到一组参数使loss值最小。<本质是训练，用已知的数据来“预测”>")])]),t._v(" "),a("hr"),t._v(" "),a("h3",{attrs:{id:"_1-function-with-unknown-parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-function-with-unknown-parameters"}},[t._v("#")]),t._v(" 1. Function with Unknown Parameters")]),t._v(" "),a("p",[t._v("​\t假设函式为  $ y=b+w*x_{1} $，这个带有Unknown的Parameter的Function叫做"),a("strong",[t._v("Model")]),t._v("。这个猜测往往就来自于你对这个问题本质上的了解，也就是 Domain knowledge")]),t._v(" "),a("ul",[a("li",[t._v("y 为预测值，本例中为明天观看此频道的总人数")]),t._v(" "),a("li",[t._v("$x_{1}$为已知的东西，叫做Feature，本例中为今天观看的人数")]),t._v(" "),a("li",[t._v("b (bias) 和 w (weight)是未知的参数，要从数据资料中学习得到")])]),t._v(" "),a("h3",{attrs:{id:"_2-define-loss-from-training-data"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-define-loss-from-training-data"}},[t._v("#")]),t._v(" 2. Define Loss from Training Data")]),t._v(" "),a("h4",{attrs:{id:"_1-loss-函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-loss-函数"}},[t._v("#")]),t._v(" (1) Loss 函数")]),t._v(" "),a("p",[t._v("​\t定义一个 Loss 函数，输入为 Model 中的未知参数 b 和 w（Parameter），输出的值表示若把这组值传给未知参数，对于计算出的预测值是好还是不好。")]),t._v(" "),a("p",[t._v("​\t对于 Loss 的计算，要从训练资料来进行。所以对于一个预测值，可以从训练资料中知道真实的结果，因而可以计算预估结果和真正结果（Label）之间的误差 e ：$e_{i} = 计算差距(y，\\hat{y} )$。可以以此方法计算训练集中的每一天的误差，从而可得 Loss 值:$L=\\frac{1}{n}\\sum_{n}e_{_n}$")]),t._v(" "),a("p",[t._v("​\t"),a("strong",[t._v("L越大,代表这组参数越不好，L越小,代表这组参数越好")])]),t._v(" "),a("h4",{attrs:{id:"_2-计算误差e"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-计算误差e"}},[t._v("#")]),t._v(" (2) 计算误差e")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210303163837725.png",alt:"image-20210303163837725"}}),t._v(" "),a("p",[t._v("​\t如上图，计算误差 e 有多种方法：MAE、MSN 、Cross-entropy（y 和 ŷ 机率分布时使用） 等。")]),t._v(" "),a("h4",{attrs:{id:"_3-error-surface"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-error-surface"}},[t._v("#")]),t._v(" (3) Error Surface")]),t._v(" "),a("p",[t._v("​\t我们可以调整不同的 w 和 b ，為不同的w跟b的组合计算它的Loss,然后就可以画出以下这一个"),a("strong",[t._v("等高线图.")]),t._v("（尝试不同的参数，计算Loss，画出来的图）")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210303170054572.png",alt:"image-20210303170054572"}}),t._v(" "),a("p",[t._v("​\t在这个等高线图上面,"),a("strong",[t._v("越偏红色系,代表计算出来的Loss越大,就代表这一组w跟b越差,如果越偏蓝色系,就代表Loss越小,就代表这一组w跟b越好")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"_3-optimization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-optimization"}},[t._v("#")]),t._v(" 3. Optimization")]),t._v(" "),a("p",[t._v("​\t解一个最佳化的问题：找一组未知参数，让 Loss 最小。本例中是找让 Loss 最小的 w* 和 b*。一个Optimization 方法："),a("strong",[t._v("Gradient Descent")]),t._v("。")]),t._v(" "),a("p",[t._v("​\t"),a("strong",[t._v("怎样找一个 w 让这个 loss 的值最小呢")]),t._v("？随机选取一个初始的点，这个初始的点,我们叫做 w0-> 求参数对 Loss 的微分 -> 更新参数值。朝低的地方跨一步，那更新时这一步要跨多大呢？这一步的步伐的大小取决于这个地方的斜率和学习率 ($\\eta $)（自己设定的）。这种你在做机器学习,需要自己设定的东西,叫做hyperparameters （超参数）。")]),t._v(" "),a("p",[a("strong",[t._v("什么时候停下来呢")]),t._v("？往往有两种状况：")]),t._v(" "),a("ul",[a("li",[t._v("第一种状况是你失去耐心了，你一开始会设定说，我今天在调整我的参数的时候，我"),a("strong",[t._v("最多计算几次")]),t._v("；")]),t._v(" "),a("li",[t._v("那还有另外一种理想上停下来的可能是，今天当我们不断调整参数时调整到一个地方，它的微分的值算出来正好是 0 的时候，如果这一项正好算出来是0.0乘上 learning rate 还是 0，所以你的参数就不会再移动位置，那参数的位置就不会再更新。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20220120152717142.png",alt:"image-20220120152717142"}})]),t._v(" "),a("p",[t._v("​\t你可能会发现 Gradient Descent 这个方法有一个巨大的问题，我们没有找到真正最好的解，我们只是找到的 local minima 而不是 global minima。其实，"),a("strong",[t._v("local minima 是一个假问题")]),t._v("，我们在做 Gradient Descent 的时候真正面对的难题不是 local minima，之后会讲到它的真正痛点在哪。")]),t._v(" "),a("p",[t._v("​\t刚刚只有一个参数 w，将其扩展至二维乃至多维是同理。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210303210927624.png",alt:"image-20210303210927624"}}),t._v(" "),a("p",[a("font",{attrs:{color:"red"}},[t._v("optimization")])],1),t._v(" "),a("h1",{attrs:{id:"linear-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#linear-model"}},[t._v("#")]),t._v(" Linear Model")]),t._v(" "),a("p",[t._v("​\t上文中的模型即"),a("u",[t._v("线性模型")]),t._v("。")]),t._v(" "),a("p",[t._v("​\t观察真实数据，发现每隔七天一个循环，因此可以修改模型（模型的修改通常来自于对问题的理解，即 Domain Knowledge）。因而模型从$y=b+wx_1$ --\x3e $y=b+ \\sum_{j=1}^{7}w_jx_j$，前七天的观看人数都被列入考虑。")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210303214752063.png",alt:"image-20210303214752063"}}),t._v(" "),a("h1",{attrs:{id:"piecewise-linear-curves-分段线性曲线"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#piecewise-linear-curves-分段线性曲线"}},[t._v("#")]),t._v(" Piecewise Linear Curves(分段线性曲线)")]),t._v(" "),a("p",[t._v("不管怎麼摆弄w 跟 b,你永远製造不出红色那一条线,你永远无法用 Linear 的 Model,製造红色这一条线。因为 model 的限制而使得永远无法模拟真实状况，这种来自 Model 的限制，叫做"),a("strong",[t._v("Model 的 Bias")]),t._v("（≠上文中 Model 中的 Bias b）。")]),t._v(" "),a("h3",{attrs:{id:"_1-红色曲线怎么表达"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-红色曲线怎么表达"}},[t._v("#")]),t._v(" 1. 红色曲线怎么表达")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210304105220371.png",alt:"image-20210304105220371"}}),t._v(" "),a("p",[t._v("​\t图中红色的这一条曲线，可以看作是一个常数，加上一群蓝色这样的 Function。\t\n"),a("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305105846439.png",alt:"image-20210305105846439"}})]),t._v(" "),a("p",[t._v("​\t这个蓝色的 Function 叫做 "),a("strong",[t._v("Hard Sigmoid")]),t._v("，它的特性是")]),t._v(" "),a("ul",[a("li",[t._v("当输入的值,当 x 轴的值小於某一个这个 Flash Hold 的时候,它是某一个定值,")]),t._v(" "),a("li",[t._v("大於另外一个 Flash Hold 的时候,又是另外一个定值,")]),t._v(" "),a("li",[t._v("中间有一个斜坡")])]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305125221397.png",alt:"image-20210305125221397"}}),t._v(" "),a("p",[t._v("​\t Piecewise Linear 的 Curves 越复杂,也就是这个转折的点越多啊,那你需要的这个蓝色的 Function 就越多。可以用 Piecewise Linear 的 Curves,去逼近任何的连续的曲线,而每一个 Piecewise Linear 的 Curves,又都可以用一大堆蓝色的 Function 组合起来,也就是说,"),a("u",[t._v("我只要有足够的蓝色 Function 把它加起来,我也许就可以变成任何连续的曲线。")])]),t._v(" "),a("h3",{attrs:{id:"_2-蓝色-function-hard-sigmoid-怎么写出来"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-蓝色-function-hard-sigmoid-怎么写出来"}},[t._v("#")]),t._v(" 2. 蓝色 Function （Hard Sigmoid）怎么写出来")]),t._v(" "),a("h4",{attrs:{id:"_1-sigmoid-function-s型曲线"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-sigmoid-function-s型曲线"}},[t._v("#")]),t._v(" （1）Sigmoid Function（S型曲线)")]),t._v(" "),a("p",[t._v("用 sigmoid Function 来逼近蓝色 Function。公式：")]),t._v(" "),a("p",[t._v("$y=c\\frac{1}{1+e^{b+wx_1}} =c*sigmoid(b+wx_1) $")]),t._v(" "),a("h4",{attrs:{id:"_2-各式各样的蓝色-function-怎样制造出来"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-各式各样的蓝色-function-怎样制造出来"}},[t._v("#")]),t._v(" (2) 各式各样的蓝色 Function 怎样制造出来")]),t._v(" "),a("p",[t._v("调整 b 、w 和 c ：")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("如果你今天改 w 你就会改变"),a("strong",[t._v("斜率")]),t._v("你就会改变斜坡的坡度")])]),t._v(" "),a("li",[a("p",[t._v("如果你动了  b 你就可以把这一个 Sigmoid Function 左右移动")])]),t._v(" "),a("li",[a("p",[t._v("如果你改 c 你就可以改变它的高度")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305132035492.png",alt:"image-20210305132035492"}}),t._v(" "),a("p",[a("u",[t._v("\t不同的 Sigmoid Function 叠起来以后，可以去逼近各种Piecewise Linear 的 Function，然后可以用来来近似不同的连续函数。")])])])]),t._v(" "),a("h3",{attrs:{id:"_3-红色-function-长什么样子"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-红色-function-长什么样子"}},[t._v("#")]),t._v(" 3. 红色 Function 长什么样子")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305134454023.png",alt:"image-20210305134454023"}}),t._v(" "),a("p",[t._v("红色曲线 = 蓝色0+1+2+3，即\n"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305135651731.png",alt:"image-20210305135651731"}})]),t._v(" "),a("h4",{attrs:{id:"如何减少-model-的bias"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#如何减少-model-的bias"}},[t._v("#")]),t._v(" 如何减少 Model 的Bias")]),t._v(" "),a("p",[t._v("写一个更有弹性的,有未知参数的 Function：")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305135850892.png",alt:"image-20210305135850892"}}),t._v(" "),a("p",[t._v("​\t此时，用了多个Feature，即$X_1$。 j 来代表 Feature 的编号.")]),t._v(" "),a("p",[t._v("​\t我们先考虑一下 j 就是 1 2 3 的状况,就是我们"),a("strong",[t._v("只考虑三个 Feature")]),t._v("。举例来说 我们只考虑前一、二、三天的 Case，")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("所以 j 等於 1 2 3,那所以输入就是 "),a("strong",[t._v("x1 代表前一天的观看人数,x2 两天前观看人数,x3 三天前的观看人数")])])]),t._v(" "),a("li",[a("p",[t._v("每一个 "),a("strong",[t._v("i 就代表了一个蓝色的 Function")]),t._v(",只是我们现在每一个蓝色的 Function,都用一个 Sigmoid Function 来比近似它，1 2 3 代表有三个 Sigmoid Function,")]),t._v(" "),a("p",[t._v("这个"),a("strong",[t._v("括号裡面")]),t._v("做的事情是什麼：")])])]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305143317386.png",alt:"image-20210305143317386"}}),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305144019248.png",alt:"image-20210305144019248"}}),t._v(" "),a("p",[t._v("表达成矩阵形式：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305145226366.png",alt:"image-20210305145226366"}}),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305145835613.png",alt:"image-20210305145835613"}}),t._v("\n这个蓝色方框做的事情："),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305150100917.png",alt:"image-20210305150100917"}}),t._v(" "),a("p",[t._v("接下来：\n"),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305150004077.png",alt:"image-20210305150004077"}})]),t._v(" "),a("p",[a("strong",[t._v("整个过程")]),t._v("做的事情为："),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305163500058.png",alt:"image-20210305163500058"}})]),t._v(" "),a("p",[t._v("**重新定义符号：**各个列向量拼在一起，统称为$\\theta$.\n"),a("img",{staticStyle:{zoom:"45%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305163746820.png",alt:"image-20210305163746820"}})]),t._v(" "),a("h1",{attrs:{id:"back-to-ml-step-2-define-loss-from-training-data"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#back-to-ml-step-2-define-loss-from-training-data"}},[t._v("#")]),t._v(" Back to ML_Step 2 :define loss from training data")]),t._v(" "),a("p",[t._v("直接"),a("strong",[t._v("用 θ 来统设所有的参数")]),t._v(",所以我们现在的 Loss Function 就变成 $L(\\theta)$.")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305212017162.png",alt:"image-20210305212017162"}}),t._v(" "),a("h1",{attrs:{id:"back-to-ml-step-3-optimization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#back-to-ml-step-3-optimization"}},[t._v("#")]),t._v(" Back to ML_Step 3: Optimization")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305212810760.png",alt:"image-20210305212810760"}}),t._v(" "),a("p",[t._v("一直按照这个图计算下去，直到 Gradient=0 向量的结果或者不想做了。")]),t._v(" "),a("p",[t._v("​\t但是实际上，我们的数据集通常会很大，假设为 N ，我们把这 N 笔资料分成一个个组，一组叫 Batch，并且只拿一个 Batch 里的 Data 算一个 Loss，把他叫做 L1。假设这个 B 够大,也许 L 跟 L1 会很接近 也说不定,所以实作上的时候,每次我们会先选一个 Batch,用这个 Batch 来算 L,"),a("strong",[t._v("根据这个 L1 来算 Gradient,用这个 Gradient 来更新参数")]),t._v(",接下来再选下一个 Batch 算出 L2,根据 L2 算出 Gradient,然后再更新参数,再取下一个 Batch 算出 L3,根据 L3 算出 Gradient,再用 L3 算出来的 Gradient 来更新参数，如下图："),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305215914525.png",alt:"image-20210305215914525"}}),t._v("\n​\t所以我们并不是拿 L 来算 Gradient，实际上我们是拿一个 Batch 算出来的 L1 L2 L3，来计算 Gradient，那把所有的 Batch 都看过一次，叫做一个 "),a("strong",[t._v("Epoch")]),t._v("，每一次更新参数叫做一次 "),a("strong",[t._v("Update")]),t._v("，Update 跟 Epoch 是不一样的东西。"),a("u",[t._v("Batch Size 也是一个 HyperParameter。")]),t._v("\n​\t"),a("strong",[t._v("每次更新一次参数叫做一次 Update,把所有的 Batch 都过一遍,叫做一个 Epoch")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"模型变型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型变型"}},[t._v("#")]),t._v(" 模型变型")]),t._v(" "),a("h4",{attrs:{id:"activation-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#activation-function"}},[t._v("#")]),t._v(" Activation Function")]),t._v(" "),a("ul",[a("li",[t._v("Sigmoid（S型）")]),t._v(" "),a("li",[t._v("ReLU（Rectified Linear Unit）")])]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305220759937.png",alt:"image-20210305220759937"}}),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305221723518.png",alt:"image-20210305221723518"}}),t._v(" "),a("p",[t._v("​\t把两个 ReLU 叠起来,就可以变成 Hard 的 Sigmoid,你想要用 ReLU 的话,就把 Sigmoid 的地方,换成$max(0, b_i+w_{ij}x_j)$.")]),t._v(" "),a("p",[t._v("​\t接下来的实验都选择用了 ReLU,显然 ReLU 比较好.原因下回分解")]),t._v(" "),a("h4",{attrs:{id:"模型改进-多做几次"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型改进-多做几次"}},[t._v("#")]),t._v(" 模型改进——多做几次")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305222735833.png",alt:"image-20210305222735833"}}),t._v(" "),a("p",[a("strong",[t._v("Deep Learning")]),t._v("：Neural  Network换个了名字，重振雄风。如下图：")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://blog-1310567564.cos.ap-beijing.myqcloud.com/img/image-20210305223838837.png",alt:"image-20210305223838837"}}),t._v(" "),a("p",[a("strong",[t._v("Over Fitting")]),t._v("：更深层次之后，在训练集上效果更好，但是预测的效果更差了。")])])}),[],!1,null,null,null);a.default=o.exports}}]);